# 数据摄取管道核心逻辑 (Ingestion Core Logic)

数据摄取子系统负责将非结构化文本转换为领域模型。整个过程由 **`PipelineManager`** 协调，采用原子化的 **管道模式 (Pipeline Pattern)** 执行。

## 1. 管道架构

管道由多个模块化的 **步骤 (Steps)** 组成，这些步骤共同操作一个共享的 **`PipelineContext`**。

### 1.1 管道上下文 (`PipelineContext`)
它是单次执行过程中的数据载体，包含：
*   **`raw_content`**: 从源文件读取的原始字符串。
*   **`logs`**: 已生成的 `DailyLog` 领域实体集合。
*   **`state`**: 运行时状态，包含映射规则与配置信息 (`ConverterConfig`)。
*   **`options`**: 命令行参数与运行选项（如是否保存 JSON 中间件）。

---

## 2. 执行步骤分解

### 第一步：文件收集 (`FileCollector`)
*   根据用户输入或配置定位目标 `.txt` 文件。
*   将物理文件内容读取到 `PipelineContext` 中待处理。

### 第二步：结构验证 (`StructureValidatorStep`)
*   进行早期的词法级检查。
*   确保文件块结构正确（年份头 -> 日期块 -> 活动列表）。
*   若语法不合规，立即中止管道。

### 第三步：核心转换 (`ConverterStep`)
*   委托内部的 **`LogProcessor`** 进行逐行解析。
*   应用 **`ActivityMapper`** 执行语义标准化和基于时长的判定规则。
*   将解析后的数据填充为 `DailyLog` 领域模型。

### 第四步：逻辑链接 (`LogicLinkerStep`)
*   执行跨天数据分析。
*   将前一天的收尾与当天的起始进行时间轴对齐。
*   智能推导并合成 `sleep_night` (深夜睡眠) 记录，填补时间真空。

### 第五步：业务验证 (`LogicValidatorStep`)
*   对重构后的时间轴执行业务规则一致性检查。
*   检测时间单调性、活动最小数量等约束，防止录入逻辑错误的数据。

### 第六步：数据持久化 (Persistence / Import)
*   将校验通过的领域模型转化为持久化模型。
*   通过 **`ImportService`** 将数据批量 commit 到 SQLite 数据库。

---

## 3. 核心算法说明

### 3.1 睡眠缝合逻辑
系统认为睡眠是两个清醒周期之间的“间隙”。
*   **公式**: `睡眠时长 = 今日起床时刻 - 昨日最后活动时刻`。
*   **跨周期处理**: 对于每月的第一天，系统会自动回溯上月最后一天的记录作为参考，确保数据的生命周期连续性。

### 3.2 24小时循环算法
系统内生支持处理 24:00 跨越。若深夜睡眠起始于 23:00 并结束于次日 07:00，时长算法会自动进行进位处理，确保计算结果为正值且逻辑准确。
